{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83739671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import splinter, beautiful soup, and other dependencies\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from os import name\n",
    "\n",
    "def generatebrowser():\n",
    "    executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "    browser = Browser('chrome', **executable_path, headless = True)\n",
    "    return browser\n",
    "\n",
    "# set up splinter & initiate headless driver for deployment\n",
    "def scrape_all():\n",
    "    browser = generatebrowser()\n",
    "    news_title, news_paragraph = mars_news(browser)\n",
    "\n",
    "    # run all scraping functions and store results in a dictionary\n",
    "    data = {\n",
    "        \"news_title\": news_title,\n",
    "        \"news_paragraph\": news_paragraph,\n",
    "        \"featured_image\": featured_image(browser),\n",
    "        \"facts\": mars_facts(),\n",
    "        \"last_modified\": dt.datetime.now(),\n",
    "        \"hemispheres\": hemi_image(browser) \n",
    "    }\n",
    "# Deliverable 2 #2 - In the def scrape_all() function in your scraping.py file, create a new dictionary in the data dictionary \n",
    "        # to hold a list of dictionaries with the URL string and title of each hemisphere image.\n",
    "    \n",
    "    # Stop webdriver and return data\n",
    "    browser.quit()\n",
    "    return data\n",
    "\n",
    "#######################################################################################\n",
    "# LOCATE THE TITLE\n",
    "#######################################################################################\n",
    "# define function so it can be called\n",
    "def mars_news(browser):    \n",
    "    # scrape mars news\n",
    "    # visit the mars nasa news site\n",
    "    url = 'https://redplanetscience.com'\n",
    "    browser.visit(url)\n",
    "\n",
    "    # optional delay for loading page\n",
    "    browser.is_element_present_by_css('div.list_text', wait_time = 1)\n",
    "\n",
    "    # set up the parser/Convert the browser html to a soup object and then quit the browser\n",
    "    html = browser.html\n",
    "    news_soup = soup(html, 'html.parser')\n",
    "    \n",
    "    #add try/except for error handling\n",
    "    try:\n",
    "        slide_elem = news_soup.select_one('div.list_text')\n",
    "        #use the parent element to find the first 'a' tag and save it as 'news_title'\n",
    "        news_title = slide_elem.find('div', class_='content_title').get_text()\n",
    "        #use the parent element to find the paragraph text\n",
    "        news_p = slide_elem.find('div', class_='article_teaser_body').get_text()\n",
    "    except AttributeError:\n",
    "        return None, None\n",
    "    return news_title, news_p\n",
    "\n",
    "######################################################################################\n",
    "#LOCATE THE IMAGE\n",
    "######################################################################################\n",
    "#define function so it can be called\n",
    "def featured_image(browser):\n",
    "    url = 'https://spaceimages-mars.com/'\n",
    "    browser.visit(url)\n",
    "\n",
    "    # find and click the full image button\n",
    "    full_image_elem = browser.find_by_tag('button')[1]\n",
    "    full_image_elem.click()\n",
    "\n",
    "    # parse the resulting html with soup\n",
    "    html = browser.html\n",
    "    img_soup = soup(html, 'html.parser')\n",
    "\n",
    "    #add try/except for error handling\n",
    "    try:\n",
    "        #find the relative image url\n",
    "        img_url_rel = img_soup.find('img', class_='fancybox-image').get('src')   \n",
    "    except AttributeError:\n",
    "        return None\n",
    "\n",
    "    # Use the base URL to create an absolute URL\n",
    "    img_url = f'https://spaceimages-mars.com/{img_url_rel}'\n",
    "    return img_url\n",
    "    \n",
    "\n",
    "######################################################################################\n",
    "#LOCATE THE FUN FACTS\n",
    "######################################################################################\n",
    "def mars_facts():\n",
    "    # Add try/except for error handling\n",
    "    try:\n",
    "        # Use 'read_html' to scrape the facts table into a dataframe\n",
    "        df = pd.read_html('https://galaxyfacts-mars.com')[0]\n",
    "    except BaseException:\n",
    "        return None\n",
    "\n",
    "    # Assign columns and set index of dataframe\n",
    "    df.columns=['Description', 'Mars', 'Earth']\n",
    "    df.set_index('Description', inplace=True)\n",
    "\n",
    "    # Convert dataframe into HTML format, add bootstrap\n",
    "    return df.to_html(classes=\"table table-stripped\")\n",
    "\n",
    "    #if running as script, print scraped data\n",
    "    # print(scrape_all())\n",
    "\n",
    "\n",
    "###############################\n",
    "# Start of Challenge\n",
    "###############################\n",
    "\n",
    "###################################################################################\n",
    "# D1: Scrape High-Resolution Marsâ€™ Hemisphere Images and Titles\n",
    "\n",
    "# Hemispheres\n",
    "###################################################################################\n",
    "\n",
    "# 1. Use browser to visit the URL \n",
    "# Deliverable 2 - #3 create a function that will scrape the hemisphere data by using your code from the \n",
    "              #   Mission_to_Mars_Challenge.py file. At the end of the function, return the scraped data as a list of \n",
    "              #   dictionaries with the URL string and title of each hemisphere image.\n",
    "\n",
    "def hemi_image(browser):\n",
    "    url = 'https://marshemispheres.com/'\n",
    "    browser.visit(url)\n",
    "\n",
    "    # 2. Create a list to hold the images and titles.\n",
    "    hemisphere_image_urls = []  \n",
    "\n",
    "# 3. Write code to retrieve the image urls and titles for each hemisphere.   \n",
    "    try:\n",
    "        for i in range(4):\n",
    "            hemisphere = {}\n",
    "            browser.find_by_css('a.product-item h3')[i].click()\n",
    "            element = browser.find_link_by_text('Sample').first\n",
    "            img_url = element['href']\n",
    "            title = browser.find_by_css(\"h2.title\").text\n",
    "            hemisphere[\"img_url\"] = img_url\n",
    "            hemisphere[\"title\"] = title\n",
    "            hemisphere_image_urls.append(hemisphere)\n",
    "            browser.back()\n",
    "    except AttributeError:\n",
    "        return None\n",
    "\n",
    "    # 4. Print the list that holds the dictionary of each image url and title.\n",
    "    return hemisphere_image_urls\n",
    "\n",
    "# 5. Quit the browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafaf3ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
